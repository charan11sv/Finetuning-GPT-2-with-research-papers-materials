{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04448cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "653b6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = load(\"new_data_512.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7af0afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109542"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str(dic['probability'][1]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4675ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_time=dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a40d96f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data=[]\n",
    "for i in range(0,5):\n",
    "    sample_data.append(dic['time series'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "766335f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data=dic['time series'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "481cb3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a474aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.clear()\n",
    "data_before.clear()\n",
    "dic.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f43337c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_before=dic.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075b239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343a600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aefbbff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|███████████████████████████████████████████████████████████████████████| 67/67 [12:04<00:00, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 2.922529515935414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        if tokenizer.pad_token_id is None:\n",
    "            tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
    "        \n",
    "        # Get input IDs and attention mask\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        # Ensure pad_token_id is set\n",
    "        if 'pad_token_id' not in inputs:\n",
    "            inputs['pad_token_id'] = self.tokenizer.eos_token_id\n",
    "        \n",
    "        return input_ids, attention_mask\n",
    "\n",
    "# Function to fine-tune the model\n",
    "def fine_tune_model(model, tokenizer, dataset, batch_size=4, num_epochs=1, learning_rate=5e-5):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for input_ids, attention_mask in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = input_ids.squeeze(dim=1).to(device)\n",
    "            attention_mask = attention_mask.squeeze(dim=1).to(device)\n",
    "            labels = input_ids.clone()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# Function to interactively test the model\n",
    "def interactively_test_model(model, tokenizer):\n",
    "    while True:\n",
    "        user_input = input(\"Enter text (or 'exit' to quit): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        input_ids = tokenizer.encode(user_input, return_tensors=\"pt\").to(device)\n",
    "        output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        print(\"Generated text:\", generated_text)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Example dataset (replace this with your own data loading logic)\n",
    "\n",
    "\n",
    "# Fine-tune the model\n",
    "dataset = TextDataset(sample_data, tokenizer)\n",
    "fine_tune_model(model, tokenizer, dataset, batch_size=2, num_epochs=1)\n",
    "\n",
    "# Interactively test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "842c6b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text (or 'exit' to quit): time series is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: time series is not a good idea, and we can't be sure.\n",
      "\n",
      "We can't.\n",
      "\n",
      "We can't be sure.\n",
      "\n",
      "We can't be sure.\n",
      "\n",
      "We can't be sure.\n",
      "\n",
      "We can't be sure.\n",
      "\n",
      "We can't be sure.\n",
      "\n",
      "We can't be sure.\n",
      "\n",
      "We can't be sure.\n",
      "\n",
      "\n",
      "We can't be sure.\n",
      "\n",
      "We can't be sure.\n",
      "We can't be sure\n",
      "Enter text (or 'exit' to quit): exit\n"
     ]
    }
   ],
   "source": [
    "interactively_test_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56131fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
